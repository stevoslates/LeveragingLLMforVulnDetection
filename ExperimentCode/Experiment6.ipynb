{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPRO Code\n",
    "#Also includes testing of APE and APO on our domain to see how they performed on our domain, didnt use them in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai_api_key = ''\n",
    "\n",
    "#reading in data\n",
    "df_cot = pd.read_csv(\"/Users/stevenslater/Desktop/FinalProject/Data/cotprompts.csv\")\n",
    "df_train = pd.read_csv(\"/Users/stevenslater/Desktop/FinalProject/Data/trainwindowsimproved.csv\")\n",
    "df_test = pd.read_csv(\"/Users/stevenslater/Desktop/FinalProject/Data/testwindowsimproved.csv\")\n",
    "#df_cot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt(question):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        answer = response['choices'][0]['message']['content']\n",
    "        tokens_used = response['usage']['total_tokens']\n",
    "        input_tokens = len(question.split()) \n",
    "        return answer, tokens_used, input_tokens\n",
    "\n",
    "    except openai.error.OpenAIError as e:\n",
    "        return f\"Error: {str(e)}\", 0, 0\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred: {str(e)}\", 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_template_ins(examples_df):\n",
    "    \n",
    "    metaPrompt = \"\"\"Your task is to generate the instruction <INS>. Below are some previous instructions with their scores. The score is the f-1 score and ranges from 0 to 1.\n",
    "\n",
    "Instruction: Review conversations to ascertain customer vulnerability by identifying explicit indications of substantial health issues affecting daily functionality, severe life disruptions such as the loss of a loved one or employment, any tangible signs of emotional or economic vulnerability, or distinct shortcomings in handling financial responsibilities or navigating digital platforms. If any such indicators are unmistakably present, reply with 'Yes'; otherwise, issue a 'No'. A clear-cut 'Yes' or 'No' answer is required solely based on these established criteria. Answer: <Yes/No>\"\n",
    "Score: 0.611\n",
    "\n",
    "Instruction: Analyze the customer conversation, and allocate a 'Yes' if it directly references any underlying medical conditions, traumatic life changes, or explicit signs of financial or emotional struggle affecting daily activities, or if there are clear indications of the customer's limited ability or confidence in managing financial tasks, technology, or essential literacy. If no such indications are present, assign a 'No'. Provide only a concise 'Yes' or 'No' determination based on these guidelines. Answer: <Yes/No>\"\n",
    "Score: 0.646\n",
    "\n",
    "Instruction: Review the dialogue to identify any explicit mentions of health conditions impacting daily functioning, recent significant life changes such as bereavement or unemployment, observable evidence of substantial emotional or financial hardship, or difficulty in understanding and handling financial or digital tasks. If any of these vulnerability indicators are present, then respond with 'Yes'; if none are observed, then respond with 'No'. A definitive binary 'Yes' or 'No' conclusion is required for classification.\n",
    "Score: 0.658\n",
    "\n",
    "Instruction: Evaluate the customer conversation and respond with 'Yes' if there's explicit mention of any health conditions, debilitating life events, evidence of financial or emotional strain indicating low resilience, or signals of limited understanding or confidence in managing their finances or everyday technology. Otherwise, respond with 'No'. A definitive 'Yes' or 'No' is required based on these stated factors.\n",
    "Score: 0.668\n",
    "\n",
    "Instruction: Review the customer's dialogue for definitive references to health complications impacting routine activities, marked life events like loss of employment or personal upheaval, tangible signs of severe emotional or financial distress, or explicit difficulties with fundamental financial, literacy, or digital skills. Confirm vulnerability with a 'Yes' if manifest; if absent, provide a 'No'. Emit an unambiguous 'Yes' or 'No' verdict accordingly.\n",
    "Score: 0.703\n",
    "\n",
    "Below are examples of the problems, we are classifying customers conversations as vulnerable or not vulnerable. Our definition of vulnerability refers to customers who, due to their personal circumstances, are especially susceptible to harm. All customers are at risk of becoming vulnerable and this risk is increased by characteristics of vulnerability related to 4 key drivers:\n",
    "Health - health conditions or illnesses that affect ability to carry out day-to-day tasks.\n",
    "Life events - life events such as bereavement, job loss or relationship breakdown.\n",
    "Resilience - low ability to withstand financial or emotional shocks.\n",
    "Capability - low knowledge of financial matters or low confidence in managing money (financial capability). Low capability in other relevant areas such as literacy, or digital skills\n",
    "\"\"\"\n",
    "\n",
    "    example_string = \"\"\n",
    "    for _, example_row in examples_df.iterrows():\n",
    "        formatted_utterance = example_row['conversation_chunks']\n",
    "        #example_string += \"\\n\" + formatted_utterance + \"\\n\" \n",
    "        ins_text = \"<INS>\"\n",
    "\n",
    "        example_string += \"\\n\" + \"Conversation: \" + formatted_utterance + \"\\n\" + ins_text + \"\\n\" + \"Answer: \" + (\"Yes\" if example_row['labels'] == 1 else \"No\") + \"\\n\"\n",
    "\n",
    "\n",
    "    instruction = \"\\n Generate an instruction that is different from all the instructions <INS> above, and has a higher score than all the instructions <INS> above. The instruction should be concise, effective, and generally applicable to the classifcation task at hand. The instruction must contain that we need a Yes or No response.\\n\"\n",
    "\n",
    "    prompt = metaPrompt + example_string + instruction\n",
    "    \n",
    "\n",
    "    #print(prompt)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api_ins(num_examples):\n",
    "    \n",
    "    results = []\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    prompt = get_template_ins(df_train.sample(num_examples))\n",
    "    print(prompt)\n",
    "        \n",
    "    \n",
    "    answer, tokens_used, input_tokens = ask_gpt(prompt)\n",
    "    total_input_tokens += input_tokens\n",
    "    total_output_tokens += tokens_used - input_tokens\n",
    "\n",
    "        \n",
    "    cost_per_1k_input_tokens = 0.01\n",
    "    cost_per_1k_output_tokens = 0.03\n",
    "    total_cost = (total_input_tokens / 1000 * cost_per_1k_input_tokens) + \\\n",
    "                 (total_output_tokens / 1000 * cost_per_1k_output_tokens)\n",
    "\n",
    "    print(f\"Total Cost: {total_cost}\")\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_api_ins(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING PROMPT INSTRUCTION (We do this in Zero-shot learning environment)\n",
    "def get_template_test(example):\n",
    "    \n",
    "    metaPrompt = \"\"\"Our definition of vulnerability refers to customers who, due to their personal circumstances, are especially susceptible to harm. All customers are at risk of becoming vulnerable and this risk is increased by characteristics of vulnerability related to 4 key drivers.\n",
    "    Health - health conditions or illnesses that affect ability to carry out day-to-day tasks.\n",
    "\n",
    "    Life events - life events such as bereavement, job loss or relationship breakdown.\n",
    "\n",
    "    Resilience - low ability to withstand financial or emotional shocks.\n",
    "\n",
    "    Capability - low knowledge of financial matters or low confidence in managing money (financial capability). Low capability in other relevant areas such as literacy, or digital skills\n",
    "    \"\"\"\n",
    "\n",
    "    conversation = \"\\n\" + \"Conversation: \" + example + \"\\n\"\n",
    "\n",
    "    #This changes for every new instruction we test\n",
    "    instruction = \"\\nExamine the customer dialogue and categorize it as 'Yes' if there are definitive indicators of health issues impacting routine tasks, significant life events that could substantially affect the customer's well-being, or clear evidence of low resilience or capability in dealing with financial, emotional, technological, or literacy challenges. If these stated indicators of vulnerability are not present, classify it as 'No'. A categorical 'Yes' or 'No' is essential, reflecting these specific vulnerability parameters.Answer: <Yes/No>\\n\"\n",
    "    prompt = metaPrompt + conversation + instruction\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api_test(sampled_df):\n",
    "    results = []\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    \n",
    "    for _, row in sampled_df.iterrows():\n",
    "        \n",
    "        \n",
    "        utterance = row['conversation_chunks']\n",
    "        prompt = get_template_test(utterance)\n",
    "        \n",
    "        answer, tokens_used, input_tokens = ask_gpt(prompt)\n",
    "        total_input_tokens += input_tokens\n",
    "        total_output_tokens += tokens_used - input_tokens\n",
    "\n",
    "        \n",
    "        answer = answer.rstrip('.').strip()\n",
    "\n",
    "        \n",
    "        answer_binary = 1 if answer.lower() == \"yes\" else 0\n",
    "        \n",
    "        #print(f\"Utterance: {utterance}\")\n",
    "        #print(f\"Actual Label: {row['labels']}\")\n",
    "        #print(f\"Answer: {answer}\")\n",
    "        \n",
    "        results.append({\n",
    "            'Data': utterance,\n",
    "            'Actual Label': row['labels'],\n",
    "            'ChatGPT Label': answer_binary\n",
    "        })\n",
    "    \n",
    "    #monitoring cost\n",
    "    cost_per_1k_input_tokens = 0.01\n",
    "    cost_per_1k_output_tokens = 0.03\n",
    "    total_cost = (total_input_tokens / 1000 * cost_per_1k_input_tokens) + \\\n",
    "                 (total_output_tokens / 1000 * cost_per_1k_output_tokens)\n",
    "\n",
    "    print(f\"Total Cost: {total_cost}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = query_api_test(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#METRICS \n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "actual_labels = results['Actual Label'].astype(int)  \n",
    "predicted_labels = results['ChatGPT Label'].astype(int)  \n",
    "\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "precision = precision_score(actual_labels, predicted_labels)\n",
    "recall = recall_score(actual_labels, predicted_labels)\n",
    "f1 = f1_score(actual_labels, predicted_labels)\n",
    "\n",
    "confusion = confusion_matrix(actual_labels, predicted_labels)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE BELOW CODE WAS USED TO TEST OTHER POSSIBLE PROMPT OPTIMISAITON BASELINES AND WAS NOT USED IN THE FINAL IMPLEMENTATION\n",
    "#WE TESTED APE AND APO TO ASSESS THEIR PERFORMANCE BEFORE CONSTRUCTING EDIPROMPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APE\n",
    "#TESTING INSTRUCTION\n",
    "#Template Construction\n",
    "def get_template_ins_ape(examples_df):\n",
    "    \n",
    "    metaPrompt = \"\"\"Below are examples of the problems, we are classifying customers conversations as vulnerable or not vulnerable. Our definition of vulnerability refers to customers who, due to their personal circumstances, are especially susceptible to harm. All customers are at risk of becoming vulnerable and this risk is increased by characteristics of vulnerability related to 4 key drivers:\n",
    "Health - health conditions or illnesses that affect ability to carry out day-to-day tasks.\n",
    "Life events - life events such as bereavement, job loss or relationship breakdown.\n",
    "Resilience - low ability to withstand financial or emotional shocks.\n",
    "Capability - low knowledge of financial matters or low confidence in managing money (financial capability). Low capability in other relevant areas such as literacy, or digital skills\n",
    "\"\"\"\n",
    "\n",
    "    example_string = \"\"\n",
    "    for _, example_row in examples_df.iterrows():\n",
    "        # Use the correct row data access methods\n",
    "        formatted_utterance = example_row['conversation_chunks']\n",
    "        #example_string += \"\\n\" + formatted_utterance + \"\\n\" \n",
    "        ins_text = \"<INS>\"\n",
    "\n",
    "        example_string += \"\\n\" + \"Conversation: \" + formatted_utterance + \"\\n\" + ins_text + \"\\n\" + (\"Yes\" if example_row['labels'] == 1 else \"No\") + \"\\n\"\n",
    "\n",
    "    #For initial\n",
    "    #instruction = \"\"\"\\nGenerate a new instruction <INS>. The instruction should be concise, effective, and generally applicable to the classifcation task at hand. The instruction must contain that we need a Yes or No response.\\n\"\"\"\n",
    "\n",
    "    #For semantic similar ins\n",
    "    instruction = \"\"\"\\nHere is the current instruction: 'Determine if the customer is 'Vulnerable' (Yes) or 'Not Vulnerable' (No) by evaluating indications of vulnerability connected to Health, Life events, Resilience, and Capability. Provide a 'Yes' response only if there are explicit signs of vulnerability in any of these areas; if not, or in doubt, respond with 'No'. Your answer should be either 'Yes' or 'No'.\\n\n",
    "Generate a new instruction <INS> that is semantically similar to the current instruction. The instruction should be concise, effective, and generally applicable to the classifcation task at hand. The instruction must contain that we need a Yes or No response.\\n\"\"\"\n",
    "\n",
    "    prompt = metaPrompt + example_string + instruction\n",
    "    \n",
    "\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api_ins_ape(num_examples):\n",
    "    results = []\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    prompt = get_template_ins_ape(df_train.sample(num_examples))\n",
    "    #print(prompt)\n",
    "        \n",
    "    \n",
    "    answer, tokens_used, input_tokens = ask_gpt(prompt)\n",
    "    total_input_tokens += input_tokens\n",
    "    total_output_tokens += tokens_used - input_tokens\n",
    "\n",
    "        \n",
    "  \n",
    "    #monitoring cost\n",
    "    cost_per_1k_input_tokens = 0.01\n",
    "    cost_per_1k_output_tokens = 0.03\n",
    "    total_cost = (total_input_tokens / 1000 * cost_per_1k_input_tokens) + \\\n",
    "                 (total_output_tokens / 1000 * cost_per_1k_output_tokens)\n",
    "\n",
    "    print(f\"Total Cost: {total_cost}\")\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_api_ins_ape(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api_test_ape(sampled_df):\n",
    "    results = []\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    for _, row in sampled_df.iterrows():\n",
    "        \n",
    "        \n",
    "        utterance = row['conversation_chunks']\n",
    "        prompt = get_template_test(utterance)\n",
    "        #print(prompt)\n",
    "\n",
    "        answer, tokens_used, input_tokens = ask_gpt(prompt)\n",
    "        total_input_tokens += input_tokens\n",
    "        total_output_tokens += tokens_used - input_tokens\n",
    "\n",
    "        # Format answer\n",
    "        answer = answer.rstrip('.').strip()\n",
    "        answer_binary = 1 if answer.lower() == \"yes\" else 0\n",
    "        \n",
    "        #print(f\"Utterance: {utterance}\")\n",
    "        #print(f\"Actual Label: {row['labels']}\")\n",
    "        #print(f\"Answer: {answer}\")\n",
    "        results.append({\n",
    "            'Data': utterance,\n",
    "            'Actual Label': row['labels'],\n",
    "            'ChatGPT Label': answer_binary\n",
    "        })\n",
    "    \n",
    "    #monitoring cost\n",
    "    cost_per_1k_input_tokens = 0.01\n",
    "    cost_per_1k_output_tokens = 0.03\n",
    "    total_cost = (total_input_tokens / 1000 * cost_per_1k_input_tokens) + \\\n",
    "                 (total_output_tokens / 1000 * cost_per_1k_output_tokens)\n",
    "\n",
    "    print(f\"Total Cost: {total_cost}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = query_api_test_ape(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#METRICS \n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "actual_labels = results['Actual Label'].astype(int)  \n",
    "predicted_labels = results['ChatGPT Label'].astype(int)  \n",
    "\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "precision = precision_score(actual_labels, predicted_labels)\n",
    "recall = recall_score(actual_labels, predicted_labels)\n",
    "f1 = f1_score(actual_labels, predicted_labels)\n",
    "\n",
    "confusion = confusion_matrix(actual_labels, predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APO - Asks LLM for text feedbakc on how to uodate and improve prompt\n",
    "#TESTING INSTRUCTION\n",
    "#Template Construction\n",
    "def get_template_ins_apo(examples_df):\n",
    "    \n",
    "    metaPrompt = \"\"\"Below are examples of the problems, we are classifying customers conversations as vulnerable or not vulnerable. Our definition of vulnerability refers to customers who, due to their personal circumstances, are especially susceptible to harm. All customers are at risk of becoming vulnerable and this risk is increased by characteristics of vulnerability related to 4 key drivers:\n",
    "Health - health conditions or illnesses that affect ability to carry out day-to-day tasks.\n",
    "Life events - life events such as bereavement, job loss or relationship breakdown.\n",
    "Resilience - low ability to withstand financial or emotional shocks.\n",
    "Capability - low knowledge of financial matters or low confidence in managing money (financial capability). Low capability in other relevant areas such as literacy, or digital skills\n",
    "\"\"\"\n",
    "\n",
    "    example_string = \"\"\n",
    "    for _, example_row in examples_df.iterrows():\n",
    "        formatted_utterance = example_row['conversation_chunks']\n",
    "        #example_string += \"\\n\" + formatted_utterance + \"\\n\" \n",
    "        ins_text = \"<INS>\"\n",
    "\n",
    "        example_string += \"\\n\" + \"Conversation: \" + formatted_utterance + \"\\n\" + ins_text + \"\\n\" + (\"Yes\" if example_row['labels'] == 1 else \"No\") + \"\\n\"\n",
    "\n",
    "    \n",
    "    instruction = \"\"\"\\nHere is the current instruction: 'Analyze each excerpt of customer conversation to identify any indicators of vulnerability, referring to the following drivers of vulnerability: Health (e.g., health conditions), Life events (e.g., bereavement), Resilience (e.g., financial or emotional shock endurance), and Capability (e.g., financial literacy, digital skills). Confirm the presence of vulnerability with 'Yes' only if there is explicit mention or strong implication of these issues. In absence of such evidence, or where information is insufficient to conclude vulnerability, respond with ‘No'. Base your judgment solely on the conversation content provided, without assuming additional context or providing explanations.\\n\n",
    "Please provide text feedback on how to update and improve the current instruction. The new instruction should be be concise, effective, and generally applicable to the classifcation task at hand.\\n\"\"\"\n",
    "\n",
    "    prompt = metaPrompt + example_string + instruction\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api_ins_apo(num_examples):\n",
    "    results = []\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    prompt = get_template_ins_apo(df_train.sample(num_examples))\n",
    "    #print(prompt)\n",
    "        \n",
    "    answer, tokens_used, input_tokens = ask_gpt(prompt)\n",
    "    total_input_tokens += input_tokens\n",
    "    total_output_tokens += tokens_used - input_tokens\n",
    "\n",
    "        \n",
    "  \n",
    "    #monitoring cost\n",
    "    cost_per_1k_input_tokens = 0.01\n",
    "    cost_per_1k_output_tokens = 0.03\n",
    "    total_cost = (total_input_tokens / 1000 * cost_per_1k_input_tokens) + \\\n",
    "                 (total_output_tokens / 1000 * cost_per_1k_output_tokens)\n",
    "\n",
    "    print(f\"Total Cost: {total_cost}\")\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_api_ins_apo(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api_test_apo(sampled_df):\n",
    "    results = []\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    \n",
    "    for _, row in sampled_df.iterrows():\n",
    "        \n",
    "        \n",
    "        utterance = row['conversation_chunks']\n",
    "        prompt = get_template_test(utterance)\n",
    "        #print(prompt)\n",
    "       \n",
    "        answer, tokens_used, input_tokens = ask_gpt(prompt)\n",
    "        total_input_tokens += input_tokens\n",
    "        total_output_tokens += tokens_used - input_tokens\n",
    "\n",
    "        \n",
    "        answer = answer.rstrip('.').strip()\n",
    "\n",
    "        \n",
    "        answer_binary = 1 if answer.lower() == \"yes\" else 0\n",
    "        \n",
    "        #print(f\"Utterance: {utterance}\")\n",
    "        #print(f\"Actual Label: {row['labels']}\")\n",
    "        #print(f\"Answer: {answer}\")\n",
    "        results.append({\n",
    "            'Data': utterance,\n",
    "            'Actual Label': row['labels'],\n",
    "            'ChatGPT Label': answer_binary\n",
    "        })\n",
    "    \n",
    "    \n",
    "    cost_per_1k_input_tokens = 0.01\n",
    "    cost_per_1k_output_tokens = 0.03\n",
    "    total_cost = (total_input_tokens / 1000 * cost_per_1k_input_tokens) + \\\n",
    "                 (total_output_tokens / 1000 * cost_per_1k_output_tokens)\n",
    "\n",
    "    print(f\"Total Cost: {total_cost}\")\n",
    "    \n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = query_api_test_apo(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#METRICS \n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "actual_labels = results['Actual Label'].astype(int) \n",
    "predicted_labels = results['ChatGPT Label'].astype(int) \n",
    "\n",
    "\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "precision = precision_score(actual_labels, predicted_labels)\n",
    "recall = recall_score(actual_labels, predicted_labels)\n",
    "f1 = f1_score(actual_labels, predicted_labels)\n",
    "\n",
    "\n",
    "confusion = confusion_matrix(actual_labels, predicted_labels)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
