{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is FEW-SHOT (answer-only) baseline experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "openai.api_key = '' #deleted for upload\n",
    "\n",
    "\n",
    "#reading in data\n",
    "df = pd.read_csv(\"/Users/stevenslater/Desktop/FinalProject/Data/trainwindowsimproved.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Template Construction\n",
    "def get_template(utterance, examples_df):\n",
    "    \n",
    "    metaPrompt = f\"\"\"Our definition of vulnerability refers to customers who, due to their personal circumstances, are especially susceptible to harm. All customers are at risk of becoming vulnerable and this risk is increased by characteristics of vulnerability related to 4 key drivers.\n",
    "    Health - health conditions or illnesses that affect ability to carry out day-to-day tasks.\n",
    "\n",
    "    Life events - life events such as bereavement, job loss or relationship breakdown.\n",
    "\n",
    "    Resilience - low ability to withstand financial or emotional shocks.\n",
    "\n",
    "    Capability - low knowledge of financial matters or low confidence in managing money (financial capability). Low capability in other relevant areas such as literacy, or digital skills\n",
    "    \n",
    "    Below is {len(examples_df)} examples of conversations with customers, where the customer is either vulnerable (yes) or not vulnerable (no), we have provided the answer to each example for help.\n",
    "    \"\"\"\n",
    "\n",
    "    example_string = \"\"\n",
    "    for _, example_row in examples_df.iterrows():\n",
    "        formatted_utterance = example_row['conversation_chunks']\n",
    "        label_text = \"Yes\" if example_row['labels'] == 1 else \"No\"\n",
    "        example_string += \"\\n\" + \"Example Conversation: \" + formatted_utterance + \"\\nAnswer: \" + label_text + \"\\n\"\n",
    "\n",
    "\n",
    "\n",
    "    question = \"\\n\" + \"Conversation to Classify: \" + utterance + \"\\n\"\n",
    "    instruction = \"\\n Using the provided examples above for help, is the customer from the above conversation vulnerable based on our definiton of vulnerability, only answer Yes or No, and you must say only Yes or No.\"\n",
    "    \n",
    "    prompt = metaPrompt + example_string + question + instruction\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt(question):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        answer = response['choices'][0]['message']['content']\n",
    "        tokens_used = response['usage']['total_tokens']\n",
    "        input_tokens = len(question.split()) \n",
    "        return answer, tokens_used, input_tokens\n",
    "\n",
    "    except openai.error.OpenAIError as e:\n",
    "        return f\"Error: {str(e)}\", 0, 0\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred: {str(e)}\", 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api(sampled_df, num_examples):\n",
    "    results = []\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    for _, row in sampled_df.iterrows():\n",
    "        \n",
    "        \n",
    "        utterance = row['conversation_chunks']\n",
    "\n",
    "        examples_df = df.sample(n=num_examples)\n",
    "        prompt = get_template(utterance, examples_df)\n",
    "       \n",
    "        answer, tokens_used, input_tokens = ask_gpt(prompt)\n",
    "        total_input_tokens += input_tokens\n",
    "        total_output_tokens += tokens_used - input_tokens\n",
    "\n",
    "        \n",
    "        answer = answer.rstrip('.').strip()\n",
    "\n",
    "        # convert the answer to a binary value \n",
    "        answer_binary = 1 if answer.lower() == \"yes\" else 0\n",
    "        \n",
    "        results.append({\n",
    "            'Data': utterance,\n",
    "            'Actual Label': row['labels'],\n",
    "            'ChatGPT Label': answer_binary\n",
    "        })\n",
    "    \n",
    "    cost_per_1k_input_tokens = 0.01\n",
    "    cost_per_1k_output_tokens = 0.03\n",
    "    total_cost = (total_input_tokens / 1000 * cost_per_1k_input_tokens) + \\\n",
    "                 (total_output_tokens / 1000 * cost_per_1k_output_tokens)\n",
    "\n",
    "    print(f\"Total Cost: {total_cost}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "df_test = pd.read_csv(\"/Users/stevenslater/Desktop/FinalProject/Data/testwindowsimproved.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test_12_examples = query_api(df_test, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "results_test_12_examples.to_csv(\"/Users/stevenslater/Desktop/FinalProject/Experiments/Experiement 4 - FSL/results_test_12_examples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#METRICS \n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "actual_labels = results_test_8_examples['Actual Label'].astype(int) \n",
    "predicted_labels = results_test_8_examples['ChatGPT Label'].astype(int) \n",
    "\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "precision = precision_score(actual_labels, predicted_labels)\n",
    "recall = recall_score(actual_labels, predicted_labels)\n",
    "f1 = f1_score(actual_labels, predicted_labels)\n",
    "\n",
    "confusion = confusion_matrix(actual_labels, predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
