{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chain of Thought Prompting Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = '' #deleted fro github\n",
    "\n",
    "\n",
    "#Reading in data\n",
    "#df_cot has the CoT prompts\n",
    "df_cot = pd.read_csv(\"/Users/stevenslater/Desktop/FinalProject/Data/cotprompts.csv\")\n",
    "df_train = pd.read_csv(\"/Users/stevenslater/Desktop/FinalProject/Data/trainwindowsimproved.csv\")\n",
    "df_test = pd.read_csv(\"/Users/stevenslater/Desktop/FinalProject/Data/testwindowsimproved.csv\")\n",
    "\n",
    "df_cot_baseline = pd.read_csv(\"/Users/stevenslater/Desktop/FinalProject/Data/cotbaselineprompts.csv\")\n",
    "#df_cot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cot_baseline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask_gpt(question):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        answer = response['choices'][0]['message']['content']\n",
    "        tokens_used = response['usage']['total_tokens']\n",
    "        input_tokens = len(question.split()) \n",
    "        return answer, tokens_used, input_tokens\n",
    "\n",
    "    except openai.error.OpenAIError as e:\n",
    "        return f\"Error: {str(e)}\", 0, 0\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred: {str(e)}\", 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_template(utterance, examples_df):\n",
    "    \n",
    "    metaPrompt = f\"\"\"Our definition of vulnerability refers to customers who, due to their personal circumstances, are especially susceptible to harm. All customers are at risk of becoming vulnerable and this risk is increased by characteristics of vulnerability related to 4 key drivers.\n",
    "    Health - health conditions or illnesses that affect ability to carry out day-to-day tasks.\n",
    "\n",
    "    Life events - life events such as bereavement, job loss or relationship breakdown.\n",
    "\n",
    "    Resilience - low ability to withstand financial or emotional shocks.\n",
    "\n",
    "    Capability - low knowledge of financial matters or low confidence in managing money (financial capability). Low capability in other relevant areas such as literacy, or digital skills\n",
    "    \n",
    "    Below is {len(examples_df)} examples of conversations with customers, where the customer is either vulnerable or not vulnerable, we have provided chain of thought reasoning to answer each example for help.\n",
    "    \"\"\"\n",
    "\n",
    "    example_string = \"\"\n",
    "    for _, example_row in examples_df.iterrows():\n",
    "        formatted_utterance = example_row['Utterance'] \n",
    "        label_text = example_row['Chainofthought']\n",
    "        example_string += \"\\n\" + \"Example Conversation: \" + formatted_utterance + \"\\nAnswer: \" + label_text + \"\\n\"\n",
    "\n",
    "    #changed for optimal prompt in final exp.\n",
    "    instruction = \"\\n Using the provided examples and their correct reasoned answers above for help, is the customer from the above conversation vulnerable based on our definiton of vulnerability, only answer Yes or No, and you must say only Yes or No.\\n\"\n",
    "    question = \"\\n\" + \"Conversation to Classify: \" + ' \"' + utterance + '\" ' + \"\\n\"\n",
    "    \n",
    "    prompt = metaPrompt + example_string + question + instruction\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api(df, num_examples):\n",
    "    \n",
    "    results = []\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        \n",
    "        \n",
    "        utterance = row['conversation_chunks']\n",
    "\n",
    "        # Define the four groups (misclassified - V/NV , gudelines - V/NV)\n",
    "        group1 = df_cot[(df_cot['Label'] == 1) & (df_cot['IsFromGuidelines'] == 1)]\n",
    "        group2 = df_cot[(df_cot['Label'] == 1) & (df_cot['IsFromGuidelines'] == 0)]\n",
    "        group3 = df_cot[(df_cot['Label'] == 0) & (df_cot['IsFromGuidelines'] == 1)]\n",
    "        group4 = df_cot[(df_cot['Label'] == 0) & (df_cot['IsFromGuidelines'] == 0)]\n",
    "\n",
    "        # Sample from each group as when needed\n",
    "        #sample1 = group1.sample(n=num_examples // 1) #// 2)\n",
    "        #sample2 = group2.sample(n=num_examples // 1)    #only two here as two groups (Pos and guideliens, Pos and no guidelines)\n",
    "        #sample3 = group3.sample(n=num_examples // 1) #// 2)\n",
    "        sample4 = group4.sample(n=num_examples // 1)\n",
    "\n",
    "        # Combine the samples\n",
    "        combined_sample = pd.concat([sample4]) #sample3, sample4\n",
    "        \n",
    "\n",
    "        # shuffle\n",
    "        combined_sample = combined_sample.sample(frac=1).reset_index(drop=True)\n",
    "        cot_examples = combined_sample\n",
    "\n",
    "        #COT BASELINE\n",
    "        #cot_examples = df_cot_baseline.sample(n=num_examples)\n",
    "        \n",
    "\n",
    "        #cot_examples = df_cot.sample(n=num_examples)\n",
    "        prompt = get_template(utterance, cot_examples)\n",
    "    \n",
    "        #query GPT\n",
    "        answer, tokens_used, input_tokens = ask_gpt(prompt)\n",
    "        total_input_tokens += input_tokens\n",
    "        total_output_tokens += tokens_used - input_tokens\n",
    "\n",
    "        # Format answer\n",
    "        answer = answer.rstrip('.').strip()\n",
    "\n",
    "        \n",
    "        answer_binary = 1 if answer.lower() == \"yes\" else 0\n",
    "\n",
    "        \n",
    "        results.append({\n",
    "            'Data': utterance,\n",
    "            'Actual Label': row['labels'],\n",
    "            'ChatGPT Label': answer_binary\n",
    "        })\n",
    "    \n",
    "    #monitoring cost\n",
    "    cost_per_1k_input_tokens = 0.01\n",
    "    cost_per_1k_output_tokens = 0.03\n",
    "    total_cost = (total_input_tokens / 1000 * cost_per_1k_input_tokens) + \\\n",
    "                 (total_output_tokens / 1000 * cost_per_1k_output_tokens)\n",
    "\n",
    "    print(f\"Total Cost: {total_cost}\")\n",
    "    \n",
    "    # Convert the results list into a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampled_df = df_test\n",
    "#Run with 12 CoT examples, on the test set - did thsi for many different experiments with the CoT strategies\n",
    "results_12_misclassified_neg = query_api(sampled_df,12)\n",
    "results_12_misclassified_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save it to csv\n",
    "results_12_misclassified_neg.to_csv('results_12_misclassified_neg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#METRICS \n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "actual_labels = results_12_misclassified_neg['Actual Label'].astype(int) \n",
    "predicted_labels = results_12_misclassified_neg['ChatGPT Label'].astype(int) \n",
    "\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "precision = precision_score(actual_labels, predicted_labels)\n",
    "recall = recall_score(actual_labels, predicted_labels)\n",
    "f1 = f1_score(actual_labels, predicted_labels)\n",
    "\n",
    "confusion = confusion_matrix(actual_labels, predicted_labels)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
