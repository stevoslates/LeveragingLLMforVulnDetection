{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "import random\n",
    "\n",
    "#API Key\n",
    "openai.api_key = '' #deleted for uploading to github\n",
    "\n",
    "\n",
    "#reading in data\n",
    "df = pd.read_csv(\"/Users/stevenslater/Desktop/FinalProject/Data/trainwindowsimproved.csv\")\n",
    "df_val = pd.read_csv(\"/Users/stevenslater/Desktop/FinalProject/Data/valwindowsimproved.csv\")\n",
    "df_test = pd.read_csv(\"/Users/stevenslater/Desktop/FinalProject/Data/testwindowsimproved.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChatGPT API function - with pricing.\n",
    "def ask_gpt(question):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        answer = response['choices'][0]['message']['content']\n",
    "        tokens_used = response['usage']['total_tokens']\n",
    "        input_tokens = len(question.split())  # approx count of input tokens\n",
    "        return answer, tokens_used, input_tokens\n",
    "\n",
    "    except openai.error.OpenAIError as e:\n",
    "        return f\"Error: {str(e)}\", 0, 0\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred: {str(e)}\", 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Template Construction\n",
    "def get_template(example):\n",
    "    \n",
    "    metaPrompt = \"\"\"Our definition of vulnerability refers to customers who, due to their personal circumstances, are especially susceptible to harm. All customers are at risk of becoming vulnerable and this risk is increased by characteristics of vulnerability related to 4 key drivers.\n",
    "    Health - health conditions or illnesses that affect ability to carry out day-to-day tasks.\n",
    "\n",
    "    Life events - life events such as bereavement, job loss or relationship breakdown.\n",
    "\n",
    "    Resilience - low ability to withstand financial or emotional shocks.\n",
    "\n",
    "    Capability - low knowledge of financial matters or low confidence in managing money (financial capability). Low capability in other relevant areas such as literacy, or digital skills\n",
    "    \"\"\"\n",
    "\n",
    "    conversation = \"\\nConversation to classify: \" + example + \"\\n\"\n",
    "\n",
    "    instruction = \"\\nFrom the above conversation, is the customer vulnerable based on our definition of vulnerability, only answer Yes or No, and you must say only Yes or No.\"\n",
    "\n",
    "    \n",
    "    prompt = metaPrompt + conversation + instruction\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_api(sampled_df):\n",
    "    \n",
    "    results = []\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    \n",
    "    for _, row in sampled_df.iterrows():\n",
    "        \n",
    "        \n",
    "        utterance = row['conversation_chunks']\n",
    "        #get prompt for that conversation\n",
    "        prompt = get_template(utterance)\n",
    "        \n",
    "        #use GPT-4 to get the answer\n",
    "        answer, tokens_used, input_tokens = ask_gpt(prompt)\n",
    "        total_input_tokens += input_tokens\n",
    "        total_output_tokens += tokens_used - input_tokens\n",
    "\n",
    "        \n",
    "        answer = answer.rstrip('.').strip()\n",
    "\n",
    "        # convert the answer to a binary value\n",
    "        answer_binary = 1 if answer.lower() == \"yes\" else 0\n",
    "        \n",
    "        \n",
    "        results.append({\n",
    "            'Data': utterance,\n",
    "            'Actual Label': row['labels'],\n",
    "            'ChatGPT Label': answer_binary\n",
    "        })\n",
    "    \n",
    "    #monitoring cost\n",
    "    cost_per_1k_input_tokens = 0.01\n",
    "    cost_per_1k_output_tokens = 0.03\n",
    "    total_cost = (total_input_tokens / 1000 * cost_per_1k_input_tokens) + \\\n",
    "                 (total_output_tokens / 1000 * cost_per_1k_output_tokens)\n",
    "\n",
    "    print(f\"Total Cost: {total_cost}\")\n",
    "    \n",
    "    # convert the results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING\n",
    "df_test = pd.read_csv(\"/Users/stevenslater/Desktop/FinalProject/Data/testwindowsimproved.csv\")\n",
    "#df_test = pd.concat([df_test.drop(['data'], axis=1), df_test['data'].apply(pd.Series)], axis=1)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test_experiment_1 =  results_test\n",
    "results_test_experiment_1.to_csv('results_test_experiment_1.csv')\n",
    "results_test_experiment_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test_experiment_1 =  pd.read_csv('results_test_experiment_1.csv')\n",
    "results_test_experiment_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#METRICS \n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "actual_labels = results_test_experiment_1['Actual Label'].astype(int)  # Convert to numeric\n",
    "predicted_labels = results_test_experiment_1['ChatGPT Label'].astype(int)  # Convert to numeric\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "precision = precision_score(actual_labels, predicted_labels)\n",
    "recall = recall_score(actual_labels, predicted_labels)\n",
    "f1 = f1_score(actual_labels, predicted_labels)\n",
    "\n",
    "# create a confusion matrix\n",
    "confusion = confusion_matrix(actual_labels, predicted_labels)\n",
    "\n",
    "# print the metrics and confusion matrix\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
